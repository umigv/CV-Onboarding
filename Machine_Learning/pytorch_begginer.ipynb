{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the advanced version of the UMARV 2024-2025 PyTorch coding checkpoint\n",
    "\n",
    "In this notebook you will be creating a basic CNN architecture that performormes image recognition. This will give you the necessary experience you need to get started designing models for the robot. \n",
    "\n",
    "# This should take around 1-3 hours\n",
    "You may look to Ryan or online resources for help if you want to meet this time. We suggest you stay away from AI tools because they will not help you learn the actual material and therefore hurting your ability to contribute to the actual robot. You are obviously welcome to go beyond this time to get more out of the checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One \n",
    "Please download the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not adjust the imports unless you add additional functionality. These should be all that you need to complete the checkpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_classes = 10):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "        Network outline\n",
    "        conv1 -> ReLU -> max pool -> conv2 -> ReLU -> max pool -> fully connected\n",
    "        \n",
    "        conv1: nn.Conv2d with these parameters: in_channels=1, out_channels=8, kernel_size=(3,3), stride=1, padding=1\n",
    "        pool1: nn.MaxPool2d with these parameters: kernel_size=(2,2), stride=(2,2)\n",
    "        conv2: nn.Conv2d with these parameters: in_channels=8, out_channels=16, kernel_size=(3,3), stride=1, padding=1\n",
    "        pool2: nn.MaxPool2d with these parameters: kernel_size=(2,2), stride=(2,2)\n",
    "        fully connection: nn.Linear with these parameters: in_features=16*7*7, out_features=num_classes\n",
    "        \n",
    "        Leave the ReLU parts for the forward function\n",
    "        '''\n",
    "        #TODO: use the outline above to create the layers of your convolutional neural network.\n",
    "        self.conv1 = \n",
    "        self.pool =         #only need one pooling layer because you can reuse them\n",
    "        self.conv2 = \n",
    "        self.fc1 = \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        In this forward function you will feed a datapoint through your network and return the value.\n",
    "        The return value is a tensor (basically just an optimized list) with 10 probabilites each giving the chance that\n",
    "        the datapoint is that class.\n",
    "        \n",
    "        For example: [0.1, 0.1, 0.4, 0.0, 0.0, 0.2, 0.0, 0.0, 0.1, 0.1] this would datapoint would be classified as a 2 becuase thats what index\n",
    "        has the highest probability. \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # here is the first example of how to pass the datapoint x into a convolutional layer and then through a ReLU activation function.\n",
    "        # TODO: Finish passing x through the neural network, making sure to go through each layer and activation function from the netowrk outline above. \n",
    "        # Then return x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Once you move into the the full connected part, you must reshape x by flattening all of the pixels \n",
    "        # into a line so it can be fed through a fully connected layer. The line below does that for you.\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x =         #pass x through full connected layer\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load in the MNIST dataset and divide it into a large training dataset and a smaller testing dataset\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True) #To Tensor for effeciency\n",
    "\n",
    "'''\n",
    "A key part of a any neural network is a dataloader. These loaders will automatically grab batches of data from the dataset. You can iterate\n",
    "through these dataloaders and therefore interate over your entire dataset in a very simple way. \n",
    "'''\n",
    "#TODO: define a variable called batch_size so that the loaders know how much data to grab at a time. A good place to start is 16, 32, or 64\n",
    "\n",
    "#TODO: Replace the None with the names of the datasets you want to use for that loader and if you want to shuffle the data or not (choose one and adjust later)\n",
    "train_loader = DataLoader(dataset=None, batch_size=batch_size, shuffle=None)\n",
    "test_loader = DataLoader(dataset=None, batch_size=batch_size, shuffle=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 epochs so far\n",
      "Completed 1 epochs so far\n",
      "Completed 2 epochs so far\n",
      "Completed 3 epochs so far\n",
      "Completed 4 epochs so far\n",
      "Completed 5 epochs so far\n",
      "Completed 6 epochs so far\n",
      "Completed 7 epochs so far\n",
      "Completed 8 epochs so far\n",
      "Completed 9 epochs so far\n"
     ]
    }
   ],
   "source": [
    "#The line below should not be adjusted and simply detects if there is a usable GPU that can be used for training.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#TODO: Please set these two variables to some generic values to start\n",
    "# Learning rate is how fast your model will learn. If its too high it will miss understand the data, and if its too low it wont learn quick enough. so start with something like 0.001 0.01\n",
    "# num epochs should stay under 20 for the sake of time, plus you shouldnt need many more than that with because the dataset is pretty simple\n",
    "learning_rate = None\n",
    "num_epochs = None\n",
    "\n",
    "#Here we are defining the model and moving it to the device we want. \n",
    "model = CNN().to(device)\n",
    "\n",
    "#You may also leave these alone\n",
    "criterion = nn.CrossEntropyLoss() #criterion is the way that we will measure the accuracy of the model throughout training process. You may research CrossEntropy on your own if you want.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam is an optimizer that will help the neural network learn over time and make minor adjustments to the network to predict better.\n",
    "\n",
    "#This is the start of the training loop, the outer loop goes through the epochs and the inner loop uses the dataloader to split up the data into batches and process them seperately\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        #Send data to the device you are using so that if you have a GPU you can use its optimization for faster training\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        #TODO: There are five main steps to train any NN or CNN and you are to place them below. Each are one line.\n",
    "        #TODO 1: feed the data through your model using the model variable that was defined earlier.\n",
    "\n",
    "        \n",
    "        #TODO 2: use the criterion variable to calculate the loss of the model. This means you must pass in the predicted values you just got and the target values given from the dataset.\n",
    "\n",
    "        \n",
    "        #TODO 3: Reset the gradient values of the optimizer using the zero_grad() function.\n",
    "\n",
    "        \n",
    "        #TODO 4: Use the loss and .backward() function to perform backpropogation which will tell the network where is needs to imporove the most to make better predictions\n",
    "\n",
    "        \n",
    "        #TODO 5: Use the optimizer and the .step() function to update parameters in the network and hepfully improve the performance\n",
    "\n",
    "        \n",
    "    print(f\"Completed {epoch} epochs so far\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Lets Evaluate the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking accuracy on training data\n",
      "Got 58026 / 60000 with accuracy 96.71\n",
      "checking accuracy on test data\n",
      "Got 9701 / 10000 with accuracy 97.01\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"checking accuracy on test data\")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): #using torch.no_grad() makes sure the network parameters are not changed while we evaluate it\n",
    "        #TODO: use the loader to loop through the features (x) and labels (y)\n",
    "            #Again moving the data to the correct device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            #TODO: feed the data through your newly trained model and set the predictions to a variable\n",
    "            \n",
    "            #TODO: Now you should have a tensor that is x.shape[0] by 10 where each row represents a data point and each column is a class. \n",
    "            # Every cell should be the probability that a certain data point is classified as the given class. So now use the .max function to find the highest probability class\n",
    "            # for each data point. Try you best with this, there are multiple ways to go about it (some in one line and some in many more). If you have tried for a while, visit \n",
    "            # HINT1 below for some help\n",
    "           \n",
    "            #TODO: calculate the number of correct predictions by comparing the predictions to the labels (y).\n",
    "            num_correct += None\n",
    "            \n",
    "            num_samples += predictions.size(0) #no need to adjust, this simply keeps track of how many samples we have evaluate thus far\n",
    "            \n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\")\n",
    "    model.train()\n",
    "    \n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "These should be used if you are unsure how to move forward on this checkpoint. These will not address all issues so feel free to ask Ryan or Matt if you are struggling with something beyond the Hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HINT1: assuming scores is your model output tensor with the probabilities. you may use the max function like this: scores.max(some_parameter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
