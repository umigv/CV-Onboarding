{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the advanced version of the UMARV 2024-2025 PyTorch coding checkpoint\n",
    "\n",
    "In this notebook you will be creating a basic CNN architecture that performormes image recognition. This will give you the necessary experience you need to get started designing models for the robot. \n",
    "\n",
    "# This should take around 1-3 hours\n",
    "You may look to Ryan or online resources for help if you want to meet this time. We suggest you stay away from AI tools because they will not help you learn the actual material and therefore hurting your ability to contribute to the actual robot. You are obviously welcome to go beyond this time to get more out of the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not adjust the imports unless you add additional functionality. These should be all that you need to complete the checkpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "N-out = ((N-in + 2p - k)/s)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_classes = 10):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "        You are free to use any architecture that you feel will work. The general outline below can be copied or you can add more or less layers if you want.\n",
    "        \n",
    "        Network outline\n",
    "        conv1 -> ReLU -> max pool -> conv2 -> ReLU -> max pool -> fully connected\n",
    "        '''\n",
    "        \n",
    "        #TODO: Define your neural network architecture code here\n",
    "        \n",
    "        #If you are struggling, go to the hints section at the bottom and look at HINT 1 and 2\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #TODO: Write the forward function here that passes x through your architecture\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 8146370.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 982885.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 8663848.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2638942.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True) #To Tensor for effeciency\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#TODO: Define the training and testing data loaders. To save time we have provided you with the MNIST dataset we will use.\n",
    "\n",
    "#If you are struggling, go to the hints section at the bottom and look at HINT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 epochs so far\n",
      "Completed 1 epochs so far\n",
      "Completed 2 epochs so far\n",
      "Completed 3 epochs so far\n",
      "Completed 4 epochs so far\n",
      "Completed 5 epochs so far\n",
      "Completed 6 epochs so far\n",
      "Completed 7 epochs so far\n",
      "Completed 8 epochs so far\n",
      "Completed 9 epochs so far\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #define device and detect if you are able to use Cuda\n",
    "\n",
    "#TODO: Create 2 variables, one for learning rate and another for epochs and assign two values you believe will be best. Keep epochs low to save time, not many should be needed\n",
    "\n",
    "#TODO: Create a model variable and assign it to a CNN class object. See HINT 4 for help\n",
    "model = CNN().to(device)\n",
    "\n",
    "\n",
    "#TODO: Define critierion and optimizer variables. See HINT 5 for help\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        #TODO: complete the training loop by completeing these 6 steps below: \n",
    "        \n",
    "        \n",
    "        #TODO 1: move data and targets to the device you defined above\n",
    "        \n",
    "        #TODO 2: pass the data through the model\n",
    "        \n",
    "        #TODO 3: Calculate the loss. See HINT 6 for help\n",
    "        \n",
    "        #TODO 4: reset optimizer gradients. See HINT 7 for help\n",
    "        \n",
    "        #TODO 5: complete backpropogration using the loss. See HINT 8 for help\n",
    "        \n",
    "        #TODO 6: Update optimizer parameters. See HINT 9 for help\n",
    "        \n",
    "        \n",
    "    print(f\"Completed {epoch+1}/{num_epochs} epochs so far\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Lets Evaluate the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking accuracy on training data\n",
      "Got 58026 / 60000 with accuracy 96.71\n",
      "checking accuracy on test data\n",
      "Got 9701 / 10000 with accuracy 97.01\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"checking accuracy on test data\")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            \n",
    "            #TODO: Complete the evaluation loop by solving the steps below:\n",
    "            \n",
    "            \n",
    "            #TODO: Move input data x and y to the device we defined earlier\n",
    "            \n",
    "            #TODO: Pass x through your model\n",
    "            \n",
    "            #TODO: Now you should have a tensor that is x.shape[0] by 10 where each row represents a data point and each column is a class. \n",
    "            # Every cell should be the probability that a certain data point is classified as the given class. So now use the .max function to find the highest probability class\n",
    "            # for each data point. Try you best with this, there are multiple ways to go about it (some in one line and some in many more). If you have tried for a while, visit \n",
    "            # HINT10 for some help\n",
    "            \n",
    "            #Keep these two lines however you may need to predictions variable name if you named it something else\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\")\n",
    "    model.train()\n",
    "    \n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "These should be used if you are unsure how to move forward on this checkpoint. These will not address all issues so feel free to ask Ryan or Matt if you are struggling with something beyond the Hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HINT1: for the full connected layer the in_channels is calculated using this formula: (number of channels * width of feature * height of feature).\n",
    "\n",
    "\n",
    "\n",
    "#HINT2: the input images are 28x28x1 pixels\n",
    "\n",
    "\n",
    "\n",
    "#HINT3: Make sure you define your own batch_size because you will need to do so in order to create a loader.\n",
    "\n",
    "\n",
    "\n",
    "#HINT4: Make sure you put the model on the device you are using with the .to() function\n",
    "\n",
    "\n",
    "\n",
    "#HINT5: CrossEntropyLoss and Adam are two good choices if you are unsure. Also make sure you pass in the correct arguments to the optimizer (double check Adam documentation if needed)\n",
    "\n",
    "\n",
    "\n",
    "#HINT6: Loss is calculated using the critierion variable directly and uses the targets (correct answers) and model output (predicted values) as input\n",
    "\n",
    "\n",
    "\n",
    "#HINT7: This is a simple function that you call the zeros the gradient (view documentation for more help)\n",
    "\n",
    "\n",
    "\n",
    "#HINT8: The back propogation is dont by using the loss variable. Basically loss.some_function()\n",
    "\n",
    "\n",
    "\n",
    "#HINT9: This part is performed using the optimizer. Basically optimizer.some_function()\n",
    "\n",
    "\n",
    "\n",
    "#HINT10: assuming scores is your model output tensor with the probabilities. you may use the max function like this: scores.max(some_parameter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
